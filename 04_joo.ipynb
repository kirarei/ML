{
 "metadata": {
  "name": "",
  "signature": "sha256:9f230b7e707ada3725810f57aa14768ea6ece6adaa4e0b3ca04646b5b6cda5fe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "#<font color = 'black'><b>[Naive Bayes]</b></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color = 'lightgray'>keywords : Bayes theorem - Naive bayesian, posterior, prior, likelihood , conditional probability..</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color = 'royalblue'><b>1. Probability, Sum rule, Joint Probability, Conditional Probability </b></font><br><br>\n",
      "\n",
      "\n",
      "<a href=\"https://www.flickr.com/photos/131520363@N08/16963036251\" title=\"1 by Hye In Lee, on Flickr\"><img src=\"https://farm9.staticflickr.com/8714/16963036251_4e071b2afc_c.jpg\" width=\"800\" height=\"270\" alt=\"1\"></a>\n",
      "\n",
      "\n",
      "\n",
      "- <b>\uc0c1\uc790\uc5d0\uc11c A \uce74\ub4dc\uac00 \ub098\uc62c \ud655\ub960 / \uc0c1\uc790\uc5d0\uc11c B \uce74\ub4dc\uac00 \ub098\uc62c \ud655\ub960 <font color = 'royalblue'><b>(Probability)</b></font>\n",
      "\n",
      "    $ P(X=A) = P(A) = 4/6 $\n",
      "    \n",
      "    $ P(X=B) = P(B) = 2/6 $\n",
      "    \n",
      "\n",
      "\n",
      "- <b><font color = 'red'> \ube68\uac04\ubcc4</font>\uc774 \ub098\uc62c \ud655\ub960 <font color = 'royalblue'><b> (Sum rule)</b></font>\n",
      "\n",
      "    $ P(Red|A)P(A)+P(Red|B)P(B)\\\\ =\\frac { 4 }{ 6 } \\times \\frac { 4 }{ 6 } +\\frac { 2 }{ 6 } \\times \\frac { 3 }{ 6 } \\\\ =\\frac { 16 }{ 36 } +\\frac { 6 }{ 36 } \\\\ =\\frac { 22 }{ 36 } \\\\ =\\frac { 11 }{ 18 } $\n",
      "  \n",
      "  \n",
      "    \n",
      "- <b>\uc8fc\uba38\ub2c8 A\uc5d0\uc11c <font color = 'red'> \ube68\uac04\ubcc4 </font>\uc774 \ub098\uc62c \ud655\ub960 <font color = 'royalblue'><b> (Conditional Probability) </b></font></b>\n",
      "\n",
      "    \uc120\ud589\uc870\uac74 : \uc8fc\uba38\ub2c8 A\uac00 \uc120\ud0dd\ub428\n",
      "    \n",
      "    $ P(Y=Red | X=A) = P(Red | A) = 4/6 $\n",
      "    \n",
      "    \n",
      "    \n",
      "- <b>\uc0c1\uc790\uc5d0\uc11c A \uce74\ub4dc\uac00 \ub098\uc624\uace0, A \uc8fc\uba38\ub2c8\uc5d0\uc11c <font color = 'red'> \ube68\uac04\ubcc4 </font>\uc774 \ub098\uc62c \ud655\ub960 <font color = 'royalblue'><b>(Joint Probability)</font></b>\n",
      "\n",
      "    $P(X=A, Y=Red) = P(Red|A)P(A)$\n",
      "\n",
      "\n",
      "##<font color = 'royalblue'><b> 2. Bayes' Rule </b></font>\n",
      "\n",
      "\n",
      "- \"\ube68\uac04\ubcc4\uc774 \ubf51\ud614\uc744\ub54c, <font color = 'red'><b>\uc774 \ube68\uac04\ubcc4\uc774 \uc5b4\ub5a4 \uc8fc\uba38\ub2c8\uc5d0\uc11c \ub098\uc654\uc744\uae4c?</b></font>\" \uc758 \ud655\ub960\uc744 \ucc3e\ub294\ub370 \ubca0\uc774\uc988 \uc815\ub9ac\uac00 \uc774\uc6a9\ub41c\ub2e4.\n",
      "- <font color = 'royalblue'><b>\uc0ac\uc804\ud655\ub960(Prior Probability)</b></font>\uacfc <font color = 'royalblue'><b>Likelihood </b></font>\ub97c \uc774\uc6a9\ud558\uc5ec <font color = 'royalblue'><b><u> \uc0ac\ud6c4\ud655\ub960(Posterior Probability)</u></b></font>\uc744 \uad6c\ud560 \uc218 \uc788\ub2e4.\n",
      "\n",
      "\n",
      "<a href=\"https://www.flickr.com/photos/131520363@N08/16778068120\" title=\"2 by Hye In Lee, on Flickr\"><img src=\"https://farm8.staticflickr.com/7584/16778068120_5159dee6ce_n.jpg\" width=\"320\" height=\"212\" alt=\"2\"></a>\n",
      "\n",
      "\n",
      "- ###<font color = 'royalblue'><b>Bayes' Rule</b></font>\n",
      "\n",
      "<a href=\"https://www.flickr.com/photos/131520363@N08/16965732175\" title=\"3 by Hye In Lee, on Flickr\"><img src=\"https://farm9.staticflickr.com/8721/16965732175_f061a27793_m.jpg\" width=\"235\" height=\"240\" alt=\"3\"></a>\n",
      "\n",
      "(1) \ube68\uac04\ubcc4\uc774 \ubf51\ud614\uc744 \ub54c, \uc774 \ube68\uac04\ubcc4\uc774 A \uc8fc\uba38\ub2c8\uc5d0\uc11c \ub098\uc654\uc744 \ud655\ub960\n",
      "\n",
      "   $ P(A|Red)=\\frac { P(Red|A)P(A) }{ P(Red) } $\n",
      "\n",
      "(2) \ube68\uac04\ubcc4\uc774 \ubf51\ud614\uc744 \ub54c, \uc774 \ube68\uac04\ubcc4\uc774 B \uc8fc\uba38\ub2c8\uc5d0\uc11c \ub098\uc654\uc744 \ud655\ub960\n",
      "\n",
      "   $ P(B|Red)=\\frac { P(Red|B)P(B) }{ P(Red) } $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <font color = 'royalblue'><b> 3. Pure, Naive, Semi-naive Bayesian </b></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Likelihood\uc744 \uc5b4\ub5a4\uc2dd\uc73c\ub85c \uacc4\uc0b0\ud558\ub290\ub0d0\uc5d0 \ub530\ub77c Pure, Naive, Semi-Naive\ub85c \uad6c\ubd84\ub41c\ub2e4.\n",
      "\n",
      "<a href=\"https://www.flickr.com/photos/131520363@N08/16785753568\" title=\"4 by Hye In Lee, on Flickr\"><img src=\"https://farm8.staticflickr.com/7587/16785753568_521744c7ce.jpg\" width=\"379\" height=\"120\" alt=\"4\"></a>\n",
      "\n",
      "- Pure : \ubaa8\ub4e0 feature\ub4e4\uc774 \uc0c1\uad00\uad00\uacc4\uac00 1\uc774\ub77c\uace0 \uac00\uc815\ud558\uace0 Likelihood\ub97c \uad6c\ud568.\n",
      "- Naive : \ubaa8\ub4e0 feature\ub4e4\uc758 \uc0c1\uad00\uad00\uacc4\uac00 0\uc774\ub77c\uace0 \uac00\uc815\ud558\uace0 Likelihood\ub97c \uad6c\ud568.\n",
      "- Semi-Naive : feature\ub4e4\uc744 \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub294\uac83\ub4e4\ub07c\ub9ac \uadf8\ub8f9\ud551\ud558\uc5ec, \uadf8\ub8f9\uc548\uc5d0\uc11c\ub294 \uc0c1\uad00\uad00\uacc4\uac00 1\uc774\ub77c\uace0 \uac00\uc815\ud558\uace0 \uacc4\uc0b0, \ub2e4\ub978 \uadf8\ub8f9\ub07c\ub9ac\ub294 \uc0c1\uad00\uad00\uacc4\uac00 0\uc774\ub77c\uace0 \uac00\uc815\ud558\uace0 Likelihood\ub97c \uad6c\ud568.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#<font color = 'dimgray'><b>[Python CODE (1) : Classify Posting \"Abusive Or Not abusive\"]</b></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Process\n",
      "<b>1. Prior Probability \uad6c\ud558\uae30 </b>\n",
      "   - $p(c_1)$ : Abusive\uc77c \ud655\ub960 \n",
      "   - $p(c_0)$ : Not abusive\uc77c \ud655\ub960\n",
      "   - $p(w)$ : word \uc758 \ud655\ub960 <br>\n",
      "   \n",
      "<b>2. Likelihood \uad6c\ud558\uae30</b>\n",
      "   - $p(w|c_1)$ : Abusive\ud55c Posting\uc77c \ub54c, word\uc758 \ud655\ub960\n",
      "   - $p(w|c_0)$ : Not abusive\ud55c Posting\uc77c \ub54c, word\uc758 \ud655\ub960\n",
      "   \n",
      "<b>3. Posterior Probability \uad6c\ud558\uae30</b>\n",
      "   - $p(c_1|w) = \\frac{p(w|c_1)p(c_1)}{p(w)}$\n",
      "   - $p(c_0|w) = \\frac{p(w|c_0)p(c_0)}{p(w)}$\n",
      "   \n",
      "<b>4. MAP(Maximum A Posteriori)\ub85c Posterior pobability\uac00 \ub354 \ub192\uc740 class\ub97c \uc120\ud0dd\ud568\n",
      "   - if $p(c_1|w) > p(c_0|w), $the class is $c_1$ <br>\n",
      "   - if $p(c_1|w) < p(c_0|w), $the class is $c_0$ <br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Posting text data \ubd88\ub7ec\uc624\uae30"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Function <font color = 'tomato'>LoadDataSet</font>\n",
      "<b><i>@ Function</b></i> : Posting \ub370\uc774\ud130\uc640 \uac01 Posting\uc774 Abusive\uc778\uc9c0, Not abusive\uc778\uc9c0 label\ud55c Vector \ubd88\ub7ec\uc634.<br>\n",
      "<b><i>@ param[out]</b></i> : Posting Data, Label Vector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadDataSet():\n",
      "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
      "                  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
      "                  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
      "                  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
      "                  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
      "                  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "    classVec = [0,1,0,1,0,1]    # 1 is abusive, 0 not\n",
      "    return postingList,classVec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import bayes\n",
      "import numpy as np\n",
      "\n",
      "PostingList, ClassVector = bayes.loadDataSet();   # Load Posting Data and ClassVector (abusive:1 / not abusive :0)\n",
      "\n",
      "print(PostingList)\n",
      "print(ClassVector)\n",
      "print(PostingList[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
        "[0, 1, 0, 1, 0, 1]\n",
        "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. VocabList \ub9cc\ub4e4\uae30"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Function <font color = 'tomato'>createVocabList</font>\n",
      "<b><i>@ Function</b></i> : PostingList\ub97c \uc77d\uc5b4\uc640\uc11c \uc720\uc77c\ud55c \ub2e8\uc5b4\ubaa9\ub85d VocabList\ub97c \uc0dd\uc131\ud568. <br>\n",
      "<b><i>@ param[in]</b></i> : PostingList (\ud1a0\ud070\ubcc4\ub85c \uc790\ub978 \ub2e8\uc5b4\ub4e4\uc758 \uc9d1\ud569, \uc911\ubcf5\ub2e8\uc5b4 \uc874\uc7ac) <br>\n",
      "<b><i>@ param[out]</b></i> : VocabList (\uc911\ubcf5\ub2e8\uc5b4\uac00 \uc5c6\uc774 \uc720\uc77c\ud55c \ub2e8\uc5b4\ub4e4\uc758 \uc9d1\ud569)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# learn textbook p.67\n",
      "\n",
      "import numpy as np\n",
      "import math\n",
      "\n",
      "\n",
      "docs       =[['my','dog','has','flea','problems','help','please'],\n",
      "             ['maybe','not','take', 'him','to','dog','park','stupid'],\n",
      "             ['my','dalmation','is','so','cute','I','love','him'],\n",
      "             ['stop','posting','stupid','worthless','garbage'],\n",
      "             ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
      "         ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "_voca=set()\n",
      "for doc in docs:\n",
      "    _voca=_voca.union(set(doc))\n",
      "    \n",
      "    print(_voca)\n",
      "    set(doc) #\ud0a4\ub9cc \uaebc\ub0b4\ub294 \uba85\ub839\uc5b4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['help', 'my', 'flea', 'problems', 'has', 'please', 'dog'])\n",
        "set(['not', 'help', 'park', 'problems', 'please', 'dog', 'to', 'stupid', 'take', 'maybe', 'flea', 'has', 'my', 'him'])\n",
        "set(['cute', 'love', 'help', 'I', 'park', 'is', 'problems', 'not', 'dalmation', 'flea', 'him', 'maybe', 'please', 'dog', 'to', 'stupid', 'so', 'take', 'has', 'my'])\n",
        "set(['cute', 'love', 'help', 'garbage', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'not', 'him', 'posting', 'worthless', 'maybe', 'please', 'dog', 'to', 'stupid', 'so', 'take', 'has', 'my'])\n",
        "set(['cute', 'love', 'help', 'garbage', 'I', 'park', 'is', 'problems', 'stop', 'not', 'dalmation', 'ate', 'flea', 'him', 'posting', 'worthless', 'licks', 'how', 'maybe', 'please', 'dog', 'to', 'stupid', 'so', 'take', 'mr', 'steak', 'has', 'my'])\n",
        "set(['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my'])\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def createVocabList(dataSet):   \n",
      "    vocabSet = set([])  # create empty set\n",
      "    for document in dataSet:    # dataSet list\uc548\uc5d0 \ub2e8\uc5b4\uac00 \uc5c6\uc744 \ub54c \uae4c\uc9c0 for loop\ub97c \ub3c8\ub2e4.\n",
      "        vocabSet = vocabSet | set(document) # union of the two sets\n",
      "    return list(vocabSet)   # \ub9cc\ub4e4\uc5b4\uc9c4 vocabSet\uc744 list \ud0c0\uc785\uc73c\ub85c \ubc18\ud658."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myVocabList = bayes.createVocabList(PostingList)\n",
      "print(myVocabList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Word vector \ub9cc\ub4e4\uae30"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Function <font color = 'tomato'>setofWords2Vec</font>\n",
      "<b><i>@ Function</b></i> :inputSet \ub0b4\uc5d0 vocabList\uc5d0 \uc788\ub294 \ub2e8\uc5b4\uac00 \uc788\uc73c\uba74 1, \uc5c6\uc73c\uba74 0\uc744 \ubc18\ud658\ud558\uc5ec word vector\ub97c \uc0dd\uc131\ud568. <br>\n",
      "<b><i>@ param[in]</b></i> : vocabList, inputSet(\ubd84\ub958\ud558\uace0\uc790 \ud558\ub294 \ubb38\uc11c, \uc5ec\uae30\uc11c\ub294 Posting) <br>\n",
      "<b><i>@ param[out]</b></i> : Word vector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def setOfWords2Vec(vocabList, inputSet):\n",
      "    returnVec = [0]*len(vocabList)  # vocabList \uae38\uc774\uc758 \ubca1\ud130 \uc0dd\uc131\n",
      "    for word in inputSet:   # \ubb38\uc11c\ub0b4\uc5d0 \uc788\ub294 \ub2e8\uc5b4 \ud55c\uac1c\uc529 \ub3c4\ub294 for loop\n",
      "        if word in vocabList:   # word\uac00 vocabList\uc548\uc5d0 \uc788\ub294 \uacbd\uc6b0\n",
      "            returnVec[vocabList.index(word)] = 1   # returnVec\uc758 vocabList.index(word)\uc5d0 \ud574\ub2f9\ud558\ub294 \uc778\ub371\uc2a4\ub97c 1\ub85c \ud574\uc90c\n",
      "        else: print \"the word: %s is not in my Vocabulary!\" % word  # vocabList \uc548\uc5d0 \uc5c6\ub294 \uacbd\uc6b0\n",
      "    return returnVec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainMat = []; # \ube48 \ub9ac\uc2a4\ud2b8\ub85c \ub9cc\ub4e4\uae30\n",
      "for postinDoc in PostingList:\n",
      "    trainMat.append(bayes.setOfWords2Vec(myVocabList, postinDoc))\n",
      "\n",
      "print trainMat[0];"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>myVocabList</b> <br>\n",
      "['cute', 'love', <font color = 'tomato'><b>'help'</b></font>, 'garbage', 'quit', 'I', <font color = 'tomato'><b>'problems'</b></font>, 'is', 'park', 'stop', <font color = 'tomato'><b>'flea'</b></font>, 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', <font color = 'tomato'><b>'has'</b></font>, 'worthless', 'ate', 'to', 'maybe', <font color = 'tomato'><b>'please'</b></font>, <font color = 'tomato'><b>'dog'</b></font>, 'how', 'stupid', 'so', 'take', 'mr', 'steak', <font color = 'tomato'><b>'my'</b></font>]\n",
      "\n",
      "\n",
      "<b>1st Post : ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'</b>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Function  <font color = 'tomato'>bagOfWords2VecMN</font>\n",
      "<b><i>@ Function</b></i> :inputSet \ub0b4\uc5d0 vocabList\uc5d0 \uc788\ub294 \ub2e8\uc5b4\uac00 \uc788\uc73c\uba74 1, \uc5c6\uc73c\uba74 0\uc744 \ubc18\ud658\ud558\uc5ec word vector\ub97c \uc0dd\uc131\ud568.<br>\n",
      "<b><i>@ param[in]</b></i> : vocabList, inputSet(\ubd84\ub958\ud558\uace0\uc790 \ud558\ub294 \ubb38\uc11c, \uc5ec\uae30\uc11c\ub294 Posting) <br>\n",
      "<b><i>@ param[out]</b></i> : Word vector\uac00 \ub098\uc624\ub294\ub370, \uc911\ubcf5\ub418\uba74 \uac12\uc774 \ub204\uc801\ub418\uc5b4 \ud45c\ud604\ub428."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bagOfWords2VecMN(vocabList, inputSet):\n",
      "    # @Function : \ub204\uc801\ub418\ub294 \uc22b\uc790\uac00 \uc801\uc6a9\ub428.\n",
      "    returnVec = [0]*len(vocabList)\n",
      "    for word in inputSet:\n",
      "        if word in vocabList:\n",
      "            returnVec[vocabList.index(word)] += 1\n",
      "    return returnVec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainMat2 = [];\n",
      "for postinDoc in PostingList:\n",
      "    trainMat2.append(bayes.bagOfWords2VecMN(myVocabList, postinDoc))\n",
      "    \n",
      "print trainMat2;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. Training "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Function <font color = 'tomato'>trainNB0</font>\n",
      "<b><i>@ Function</b></i> : Likelihood\ub97c \uad6c\ud568<br>\n",
      "<b><i>@ param[in]</b></i> : trainMatrix(word vector), trainCategory(class vector)<br>\n",
      "<b><i>@ param[out]</b></i> : $p(w|c_0)$ , $p(w|c_1)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainNB0(trainMatrix, trainCategory):\n",
      "    numTrainDocs = len(trainMatrix) # train \uc2dc\ud0ac \ubb38\uc11c\uc758 \uc218\n",
      "    numWords = len(trainMatrix[0])  # trainMatrxi[0]\uc758 \uac1c\uc218\ub97c \uad6c\ud568\n",
      "    pAbusive = sum(trainCategory)/float(numTrainDocs)   # trainCategory \uc548\uc5d0 abusive \ubb38\uc11c\ub85c \ubd84\ub958\ub41c \uac83\uc740 1\ub85c \ub418\uc5b4\uc788\uae30 \ub54c\ubb38\uc5d0, \uc804\uccb4\ub85c \ub098\ub220\uc8fc\uba74 Abusive\uc758 \ud655\ub960\uc774 \ub098\uc634\n",
      "    pnonAbusive = 1 - pAbusive;\n",
      "    # \ud655\ub960\ub4e4\uc744 \uacf1\ud560 \ub54c, \uc774 \uc911 \ud558\ub098\ub77c\ub3c4 0\uc774\ub418\uba74 \uacb0\uacfc\uac00 0\uc774 \ub418\ubbc0\ub85c \uc774\ub7ec\ud55c \uc601\ud5a5\ub825\uc744 \uc904\uc774\uae30 \uc704\ud574 \ubc1c\uc0dd\ud558\ub294 \ub2e8\uc5b4\uc758 \uac1c\uc218\ub97c \ubaa8\ub450 1\ub85c \ucd08\uae30\ud654\ud558\uace0, \ubd84\ubaa8\ub294 2\ub85c \ucd08\uae30\ud654\ud55c\ub2e4.#\n",
      "    p0Num = ones(numWords); p1Num = ones(numWords)      # \ubd84\uc790\ucd08\uae30\ud654 vector\n",
      "    p0Denom = 2.0; p1Denom = 2.0                        # \ubd84\ubaa8\ucd08\uae30\ud654 scalar, \ud655\ub960\uc774\ubbc0\ub85c 0\uc774 \ub4e4\uc5b4\uac08 \uac00\ub2a5\uc131\uc744 \uc81c\uac70\ud558\uae30 \uc704\ud574 2 \uc0bd\uc785 - \uc544\ubb34\uc22b\uc790\ub098 \uc0c1\uad00\uc5c6\uc74c\n",
      "\n",
      "    for i in range(numTrainDocs):   # \ud6c8\ub828\ud560 \ubb38\uc11c\uc758 \uac1c\uc218\ub9cc\ud07c loop \n",
      "        if trainCategory[i] == 1:   # \ub9cc\uc57d \ubb38\uc11c\uac00 abusive \ubb38\uc11c\uc77c \uacbd\uc6b0 \n",
      "            p1Num += trainMatrix[i] # p1Num \ubd84\uc790\uc5d0 i\ubc88\uc9f8 \ubb38\uc11c \uc548\uc758 1\uac2f\uc218\ub9cc\ud07c \ud574\ub2f9 vocab\uc758 \uc218\uac00 \ub204\uc801\ub428.\n",
      "            p1Denom += sum(trainMatrix[i])  # p1Denom \ubd84\ubaa8\uc5d0 i\ubc88\uc9f8 \ubb38\uc11c\uc548\uc758 1\uac2f\uc218\ub9cc\ud07c \uc99d\uac00\ud568\n",
      "        else:                       # \ubb38\uc11c\uac00 non-abusive\uc778 \uacbd\uc6b0\n",
      "            p0Num += trainMatrix[i] \n",
      "            p0Denom += sum(trainMatrix[i])\n",
      "    # underflow problem : \uc791\uc740 \uc218\ub07c\ub9ac \ub108\ubb34 \ub9ce\uc774 \uacf1\ud574\uc838\uc11c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub85c \ubd80\uc815\ud655\ud55c \ub2f5\uc744 \uc0b0\ucd9c\ud558\uac8c \ub41c\ub2e4. (\ud30c\uc774\uc36c\uc5d0\uc11c\ub294 \uc791\uc740\uc218\ub97c \ub9ce\uc774 \uacf1\ud558\uba74 0\uc73c\ub85c \ubc18\uc62c\ub9bc\ud574\ubc84\ub9b0\ub2e4.) \n",
      "    # \uc774\uac83\uc744 \ub9c9\uae30 \uc704\ud574 \uacb0\uacfc\uc5d0 \ub300\ud574 \uc790\uc5f0\ub85c\uadf8\ub97c \uacc4\uc0b0\ud558\uba74 ln(a x b)=ln(a) + ln(b) \uc640 \uac19\uc73c\ubbc0\ub85c \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\ub2e4.\n",
      "    p1Vect = log(p1Num/p1Denom)         # \ubb38\uc11c\uac00 Abusive \uc77c\ub54c, \ub2e8\uc5b4\uc758 \ud655\ub960\n",
      "    p0Vect = log(p0Num/p0Denom)         # \ubb38\uc11c\uac00 not abusive \uc77c\ub54c,\ub2e8\uc5b4\uc758 \ud655\ub960 \n",
      "    return p0Vect, p1Vect, pAbusive"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "p0V, p1V, pAb = bayes.trainNB0(np.array(trainMat), np.array(ClassVector))\n",
      "print(p0V)\n",
      "print(p1V)\n",
      "print(pAb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
        " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
        " -2.56494936 -3.25809654 -3.25809654 -2.15948425 -3.25809654 -3.25809654\n",
        " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
        " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n",
        " -2.56494936 -1.87180218]\n",
        "[-3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
        " -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
        " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -2.35137526\n",
        " -3.04452244 -1.94591015 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
        " -1.94591015 -3.04452244 -1.65822808 -3.04452244 -2.35137526 -3.04452244\n",
        " -3.04452244 -3.04452244]\n",
        "0.5\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5. Classify \ud558\uae30."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Function <font color = 'tomato'>classifyNB</font>\n",
      "<b><i>@ Function</b></i> : MAP \ubc29\uc2dd, Posterior probability\uac00 \ub354 \ud070 \ucabd\uc73c\ub85c \ubd84\ub958\ud568. <br>\n",
      "<b><i>@ param[in]</b></i> : vec2Classify(\ubd84\ub958\ud560 word vector), p0Vec, p1Vec, pClass1<br>\n",
      "<b><i>@ param[out]</b></i> : \ubd84\ub958\ub41c \uacb0\uacfc"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import bayes\n",
      "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):   \n",
      "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
      "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
      "    print 'p1(Abusive) : %f\\n' %p1\n",
      "    print 'p2(Not abusive) : %f\\n' %p0\n",
      "    if p1 > p0:\n",
      "        return 1\n",
      "    else: \n",
      "        return 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testEntry = ['love', 'my', 'dalmation']\n",
      "thisDoc = np.array(bayes.setOfWords2Vec(myVocabList, testEntry))\n",
      "print(myVocabList)\n",
      "print(thisDoc)\n",
      "# \ubd84\ubaa8\ub294 \ub3d9\uc77c\ud574\uc11c, \ubd84\uc790\ub9cc \uac00\uc9c0\uace0 \uacc4\uc0b0\ud558\uc5ec  \ub85c\uadf8\uac12\uc744 \ucc98\ub9ac\ud558\ub354\ub77c\ub3c4 \ud655\ub960\uac12\uc774 - \n",
      "\n",
      "if classifyNB(thisDoc,p0V, p1V, pAb) == 0:\n",
      "    print testEntry, 'Classified as not Abusive.\\n'\n",
      "else:\n",
      "    print testEntry, 'Classified as Abusive.\\n'\n",
      "\n",
      "testEntry = ['stupid', 'garbage']\n",
      "thisDoc = np.array(bayes.setOfWords2Vec(myVocabList, testEntry))\n",
      "\n",
      "if classifyNB(thisDoc,p0V, p1V, pAb) == 0:\n",
      "    print testEntry, 'Classified as not Abusive.\\n'\n",
      "else:\n",
      "    print testEntry, 'Classified as Abusive.\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
        "[0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
        "p1(Abusive) : -9.826714\n",
        "\n",
        "p2(Not abusive) : -7.694848\n",
        "\n",
        "['love', 'my', 'dalmation'] Classified as not Abusive.\n",
        "\n",
        "p1(Abusive) : -4.702751\n",
        "\n",
        "p2(Not abusive) : -7.209340\n",
        "\n",
        "['stupid', 'garbage'] Classified as Abusive.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " #\uccab\uc9f8, \ud6c8\ub828\ub2e8\uacc4 train\n",
      "# 1. Data read\n",
      "# word vector\n",
      "import numpy as np\n",
      "import math\n",
      "docs=[['my','dog','has','flea','problems','help','please'],\n",
      "     ['maybe','not','take', 'him','to','dog','park','stupid'],\n",
      "     ['my','dalmation','is','so','cute','I','love','him'],\n",
      "     ['stop','posting','stupid','worthless','garbage'],\n",
      "     ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
      "     ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "_voca=set()\n",
      "for doc in docs:\n",
      "    _voca=_voca.union(set(doc))\n",
      "voca=dict()\n",
      "\n",
      "print(voca)\n",
      "\n",
      "for word in _voca:\n",
      "    voca[word]=len(voca)\n",
      "nword=len(voca)#32\n",
      "ndoc=len(docs) #6\n",
      "nword_doc=np.zeros([nword,ndoc]) #\uad50\uc7ac trainMat\n",
      "#docs: list\n",
      "#voca: list\n",
      "def set_nword_doc(docs,voca):\n",
      "    for di in xrange(len(docs)):\n",
      "        #print docs[di]\n",
      "        for word in docs[di]:\n",
      "            if word in voca.keys():\n",
      "                #print \" \",voca[word],word\n",
      "                nword_doc[voca[word],di]+=1\n",
      "set_nword_doc(docs,voca)\n",
      "\n",
      "def printM(ndict,ndictBym):\n",
      "    for i,j in ndict.items():\n",
      "        print \"%20s-%s\" % (i,ndictBym[j])\n",
      "#printM(voca,nword_doc)\n",
      "\n",
      "# class labels\n",
      "ndoc_class=np.zeros([ndoc,1])\n",
      "ndoc_class[1]=1\n",
      "ndoc_class[3]=1\n",
      "ndoc_class[5]=1\n",
      "#print ndoc_class.T #array([[ 0.,  1.,  0.,  1.,  0.,  1.]])\n",
      "#nclass=2\n",
      "#nword_class=np.zeros([nword,nclass])\n",
      "\n",
      "# 2. prior = \ud074\ub798\uc2a4\uac00 1\uc778 \uac2f\uc218\ub97c \uad6c\ud574\uc11c \uc804\uccb4\uac2f\uc218\ub85c \ub098\ub204\uc5b4 \uad6c\ud55c \ud655\ub960\n",
      "pAbusive=len(ndoc_class[ndoc_class[:,0]==1])/float(len(ndoc_class[:,0]))\n",
      "\n",
      "# 3. likelihood update\n",
      "# ndoc_class[:,0]==1 #split only class==1\n",
      "nword_doc_0=nword_doc[:,ndoc_class[:,0]==0]\n",
      "nword_doc_1=nword_doc[:,ndoc_class[:,0]==1]\n",
      "p0denom=nword_doc_0.sum() #19\n",
      "p1denom=nword_doc_1.sum() #24\n",
      "p0num=nword_doc_0.sum(axis=1)\n",
      "p1num=nword_doc_1.sum(axis=1)\n",
      "\n",
      "print p0denom, p1denom\n",
      "print p0num, p1num\n",
      "#p0Num=[1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 2 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 3]\n",
      "#p1Num=[0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 2 0 1 1 0 2 0 3 0 1 0 0 0]\n",
      "p0vect=p0num/p0denom #see if this equals to \uad50\uc7ac p.71\n",
      "p1vect=p1num/p1denom\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{}\n",
        "24.0 19.0\n",
        "[ 1.  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0.  0.  2.  0.  0.\n",
        "  1.  0.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  1.  3.] [ 0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.\n",
        "  0.  2.  0.  1.  1.  0.  2.  0.  3.  0.  1.  0.  0.  0.]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 6. Test\ud574\ubcf4\uae30"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#<font color = 'dimgray'><b>[Python CODE (2) : Classify Spam Email]</b></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SpamTest\n",
      "- spam\uc778 \uacbd\uc6b0 class1, spam\uc774 \uc544\ub2cc\uacbd\uc6b0 class2\n",
      "- Training data\uc640 Test data\uac00 \uc784\uc758\ub85c \uc120\ud0dd\ub428\n",
      "- Training set\uacfc Test set\uc744 \uc704\ud574 \ub370\uc774\ud130\uc758 \uc77c\ubd80\ubd84\uc744 \uc784\uc758\ub85c \uc120\ud0dd\ud558\ub294 \uac83\uc744 \ud640\ub4dc\uc544\uc6c3(hold-out) \uad50\ucc28\uac80\uc99d\uc774\ub77c\uace0 \ud55c\ub2e4."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. Text token \ub9cc\ub4e4\uae30"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docList=[]; classList = []; fullText =[]\n",
      "# Load and parse text files #\n",
      "for i in range(1,26):\n",
      "    wordList = bayes.textParse(open('email/spam/%d.txt' % i).read())\n",
      "    docList.append(wordList) # \ud30c\uc77c \uc774\uc5b4\ubd99\uc774\uae30\n",
      "    fullText.extend(wordList)\n",
      "    classList.append(1) \n",
      "    \n",
      "    wordList = bayes.textParse(open('email/ham/%d.txt' % i).read())\n",
      "    docList.append(wordList)\n",
      "    fullText.extend(wordList)\n",
      "    classList.append(0) \n",
      "print len(classList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Function <font color = 'tomato'>textParse</font>\n",
      "<b><i>@ Function</b></i> :  \ud14d\uc2a4\ud2b8\ud30c\uc77c\uc744 \ud1a0\ud070 \ubca1\ud130\ub85c \ub9cc\ub4e4\uc5b4\uc90c.<br>\n",
      "<b><i>@ param[in]</b></i> : bigString (\ubb38\uc790\uc5f4\ub85c \ub41c \ud14d\uc2a4\ud2b8\ud30c\uc77c)<br>\n",
      "<b><i>@ param[out]</b></i> : \ud1a0\ud070\ubcc4\ub85c \uc798\ub9b0 \uc0c1\ud0dc"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def textParse(bigString):    # input is big string, #output is word list\n",
      "    import re\n",
      "    listOfTokens = re.split(r'\\W*', bigString)    #w \ub294 \ubb38\uc790, (*)\uc740 \ubb38\uc790\uc218\ub9cc\ud07c \ub300\uc751\uc2dc\ud0a8\ub2e4.\n",
      "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] # \ud1a0\ud070\ubcc4\ub85c \uc798\ub77c\ub0b4\uba70 2\uae00\uc790 \uc774\uc0c1\ub9cc \uc800\uc7a5, \ub300\ubb38\uc790\ub294 \uc18c\ubb38\uc790\ub85c \ubc14\ub00c\uc5b4\uc11c \uc800\uc7a5."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Original Text</b> <br>\n",
      "--- Codeine 15mg -- 30 for $203.70 -- VISA Only!!! --\n",
      "\n",
      "-- Codeine (Methylmorphine) is a narcotic (opioid) pain reliever\n",
      "-- We have 15mg & 30mg pills -- 30/15mg for $203.70 - 60/15mg for $385.80 - 90/15mg for $562.50 -- VISA Only!!! ---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordList = bayes.textParse(open('email/spam/1.txt').read())\n",
      "print(wordList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['codeine', '15mg', 'for', '203', 'visa', 'only', 'codeine', 'methylmorphine', 'narcotic', 'opioid', 'pain', 'reliever', 'have', '15mg', '30mg', 'pills', '15mg', 'for', '203', '15mg', 'for', '385', '15mg', 'for', '562', 'visa', 'only']\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. VocabList \ub9cc\ub4e4\uae30"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabList = bayes.createVocabList(docList)    # create vocabulary\n",
      "print(vocabList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['all', 'code', 'chinese', 'focus', 'narcotic', 'supplement', 'follow', '14th', 'issues', 'increase', 'glimpse', 'foaming', 'father', 'program', 'sorry', 'insights', 'sent', '50mg', 'risk', 'far', 'of_penisen1argement', 'wallets', 'fractal', 'tickets', 'school', 'doggy', 'level', 'louis', 'having', 'upload', 'ryan', 'try', 'item', 'team', 'guy', '292', 'commented', '291', 'enjoy', 'advocate', 'talked', '199', 'concise', '195', 'horn', 'past', 'titles', 'design', 'download', 'carlo', 'blue', 'what', 'assistance', 'selected', 'while', '0nline', 'thickness', 'access', 'reply', 'net', 'viagranoprescription', 'told', 'supporting', 'full', 'zach', 'here', 'hours', 'let', 'free', 'groups', 'address', '100', 'softwares', 'great', 'incoming', 'forum', 'canadian', 'dusty', 'items', 'inspired', 'experience', 'credit', 'products', 'severepain', 'pick', 'changes', '430', 'via', 'private', 'creative', 'arolexbvlgari', 'support', 'fbi', 'hotels', 'derivatives', 'use', 'from', 'would', 'yourpenis', 'two', 'bike', 'call', 'survive', 'today', 'more', 'chapter', 'kerry', 'pretty', 'ofejacu1ate', 'moneyback', 'phone', 'train', 'dior', 'hold', 'must', 'town', 'tokyo', 'this', 'car', 'work', 'cat', 'can', 'learn', 'meet', 'example', 'control', 'share', 'accept', 'high', 'heard', 'want', 'serial', 'keep', 'information', 'winter', 'ma1eenhancement', 'located', 'amazing', 'answer', 'instead', 'cuda', 'monte', 'rude', 'huge', 'may', 'plane', 'designed', 'improving', 'such', '50092', 'natural', 'effective', 'inform', 'cannot', 'ultimate', 'fundamental', 'nvidia', 'cs5', 'lined', 'thirumalai', 'order', 'harderecetions', 'photoshop', 'help', 'office', 'over', 'logged', 'held', '130', 'through', 'cold', '138', '492', 'style', 'group', 'thank', 'how', 'brained', 'interesting', 'writing', 'window', 'mail', 'holiday', '2011', '2010', 'eugene', 'then', 'them', 'good', 'finance', 'food', 'docs', 'safe', 'runs', 'lunch', 'ferguson', 'they', 'not', 'guaranteeed', 'now', 'day', 'articles', 'february', 'name', 'permanantly', 'scenic', 'phentermin', 'rock', 'went', 'development', 'adobe', 'significantly', 'doing', 'series', 'tiffany', 'yeah', 'wednesday', 'each', 'used', '396', 'year', 'girl', 'pills', 'out', '100m', 'network', 'since', 'hangzhou', 'looking', 'specifications', 'encourage', 'shipment', 'got', 'sliding', 'model', '588', 'york', 'members', 'hermes', 'acrobat', 'certified', 'care', 'thread', 'launch', 'automatic', 'could', 'programming', 'days', '10mg', 'thing', 'length', 'place', 'invitation', '513', 'turd', 'think', 'major', 'features', '625', 'knocking', 'number', 'one', 'featured', 'done', 'specifically', 'another', 'wasn', 'owner', 'art', 'message', 'quality', 'millions', 'differ', 'management', 'service', 'top', 'moderately', 'least', 'parallel', 'needed', '180', 'station', 'listed', 'analgesic', 'gpu', 'john', 'store', 'too', 'hommies', 'courier', 'that', 'tool', 'hotel', 'lists', 'forward', 'regards', 'butt', 'focusing', 'copy', 'than', 'reliever', 'jquery', 'enabled', '5mg', 'cost', 'expo', 'cards', 'and', 'tabs', 'codeine', 'pro', 'buy', 'rent', 'have', 'need', 'aged', 'saw', 'any', 'wilmott', 'wilson', 'genuine', 'storage', 'ideas', 'note', 'also', 'take', 'doors', '325', 'brand', 'worldwide', '200', '203', 'sure', 'pain', 'price', 'who', 'watchesstore', 'most', 'visa', 'germany', 'pages', 'delivery', 'don', 'windows', '66343', 'definitely', 'professional', 'mailing', 'microsoft', 'came', 'gain', 'speedpost', 'edit', 'cheap', 'troy', '25mg', 'fine', 'find', 'based', 'jay', 'jar', 'enough', 'should', 'only', 'going', 'arvind', 'riding', 'hope', 'giants', 'his', 'gains', 'get', 'express', 'coast', '120', 'famous', 'watson', 'ones', 'new', '129', 'earn', 'freeviagra', 'biggerpenis', 'bad', 'stuff', 'ems', 'release', 'where', 'inside', 'wrote', 'view', 'mandatory', '174623', 'recieve', 'see', 'computer', '1924', 'are', 'discussions', 'close', 'said', 'jqplot', 'pictures', 'away', 'please', 'sites', 'superb', 'whybrew', 'fans', 'mandarin', '366', 'approach', 'email', 'tent', 'explosive', 'bargains', 'nature', 'amex', 'job', '385', 'come', 'both', 'plugin', 'jocelyn', 'haloney', 'connection', 'china', 'com', 'grounds', 'comment', 'exhibit', 'methylmorphine', 'drunk', 'color', 'sky', '570', '100mg', 'noprescription', '30mg', 'bathroom', 'yay', 'roofer', 'create', 'computing', 'been', 'strategy', 'attaching', 'much', 'treat', 'borders', 'prototype', '15mg', 'website', 'life', 'received', 'drugs', 'thousand', 'gas', 'modelling', 'proven', 'prices', 'assigning', '300x', 'those', 'has', 'these', 'might', 'will', 'gucci', '90563', 'expertise', 'leaves', 'julius', '322', 'femaleviagra', 'site', 'linkedin', 'jpgs', 'hydrocodone', 'experts', 'ready', 'trusted', 'bags', 'dhl', 'doctor', 'perhaps', 'suggest', 'tour', 'same', 'trip', 'python', 'party', 'web', 'dozen', 'status', 'extended', 'http', 'yesterday', 'rain', 'cheers', 'running', 'uses', 'changing', 'opportunity', 'incredib1e', 'off', 'functionalities', 'well', 'customized', 'thought', 'contact', 'quantitative', 'the', 'cca', 'latest', 'things', 'just', 'scifinance', 'being', 'money', 'mandelbrot', 'competitive', '219', 'shape', 'reputable', 'thanks', 'questions', 'behind', 'october', 'jose', 'retirement', 'had', 'capabilities', 'pharmacy', 'source', 'add', 'transformed', 'location', 'grow', 'save', 'pill', 'finder', 'bin', 'oris', 'easily', 'ambiem', 'couple', 'online', 'possible', 'vivek', 'game', 'discreet', 'know', 'mom', 'using', 'accepted', 'like', 'naturalpenisenhancement', 'success', 'ordercializviagra', 'safest', 'mathematician', 'either', 'night', 'brandviagra', 'works', 'endorsed', 'page', 'opioid', 'province', 'www', 'because', 'methods', 'creation', 'some', 'back', 'storedetailview_98', 'pricing', 'home', 'peter', 'ups', 'withoutprescription', 'strategic', 'faster', 'for', 'notification', 'decision', '86152', 'per', 'fedex', 'everything', 'does', 'moderate', 'core', 'inconvenience', 'pls', 'business', 'stepp', 'discount', 'oem', 'thailand', 'includes', 'vuitton', 'about', 'working', 'favorite', 'shipping', 'magazine', 'plus', 'cartier', 'intenseorgasns', 'automatically', 'warranty', 'mba', 'google', 'financial', 'your', 'often', 'betterejacu1ation', 'museum', 'there', 'fast', 'approved', 'generation', 'cats', 'low', 'way', 'zolpidem', 'herbal', 'was', 'files', '750', 'inches', '562', 'but', 'sophisticated', 'volume', 'link', 'announcement', '225', 'with', 'bettererections', 'fda', 'made', 'pavilion', 'wholesale', 'sounds', 'percocet', 'signed', 'placed', 'below', 'fermi', 'spaying', '156', 'tesla', 'watches', 'right', 'file', 'door', 'hamm', 'check', '119', 'benoit', 'when', 'longer', 'book', 'vicodin', 'income', 'mathematics', 'you', 'requested', 'generates', 'knew', 'class', 'welcome', 'update', 'prepared', 'chance', 'important', 'brands', 'reservation', 'died', 'individual', 'jewerly', 'required', 'buyviagra', '2007', 'time', 'starting', 'hello', 'others', 'once']\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. TestSet \ub9cc\ub4e4\uae30 (hold-out Cross validation)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainingSet = range(50); testSet=[];          # create test set\n",
      "for i in range(10):\n",
      "    randIndex = int(np.random.uniform(0,len(trainingSet)))\n",
      "    testSet.append(trainingSet[randIndex])\n",
      "    del(trainingSet[randIndex]) \n",
      "\n",
      "print(testSet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[7, 46, 15, 35, 18, 32, 8, 20, 5, 28]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4.VocabList \ub9cc\ub4e4\uae30"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainMat=[]; trainClasses = []\n",
      "\n",
      "for docIndex in trainingSet:    # train the classifier (get probs) trainNB0\n",
      "    trainMat.append(bayes.bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
      "    trainClasses.append(classList[docIndex])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5. Training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p0V,p1V,pSpam = bayes.trainNB0(np.array(trainMat),np.array(trainClasses))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(p0V)\n",
      "print(p1V)\n",
      "print(pSpam)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-5.54647868 -5.03565306 -5.95194379 -5.95194379 -6.64509097 -6.64509097\n",
        " -5.95194379 -5.95194379 -5.95194379 -6.64509097 -5.95194379 -5.95194379\n",
        " -5.95194379 -5.95194379 -5.95194379 -5.95194379 -6.64509097 -6.64509097\n",
        " -5.54647868 -6.64509097 -6.64509097 -6.64509097 -5.95194379 -5.95194379\n",
        " -5.95194379 -5.95194379 -5.95194379 -6.64509097 -6.64509097 -5.54647868\n",
        " -5.54647868 -5.54647868 -6.64509097 -5.95194379 -5.95194379 -6.64509097\n",
        " -5.03565306 -6.64509097 -5.95194379 -5.95194379 -6.64509097 -6.64509097\n",
        " -5.95194379 -6.64509097 -5.95194379 -5.95194379 -6.64509097 -5.95194379\n",
        " -5.54647868 -5.95194379 -5.95194379 -6.64509097 -6.64509097 -6.64509097\n",
        " -5.95194379 -6.64509097 -6.64509097 -5.54647868 -5.54647868 -6.64509097\n",
        " -6.64509097 -5.95194379 -5.95194379 -6.64509097 -5.54647868 -5.95194379\n",
        " -6.64509097 -5.95194379 -6.64509097 -5.03565306 -6.64509097 -6.64509097\n",
        " -6.64509097 -6.64509097 -6.64509097 -5.95194379 -6.64509097 -5.95194379\n",
        " -6.64509097 -5.95194379 -6.64509097 -6.64509097 -5.95194379 -6.64509097\n",
        " -5.95194379 -5.95194379 -6.64509097 -6.64509097 -6.64509097 -5.95194379\n",
        " -6.64509097 -4.8533315  -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -4.8533315  -6.64509097 -6.64509097 -5.95194379 -6.64509097 -5.95194379\n",
        " -5.95194379 -5.54647868 -5.25879661 -5.95194379 -5.54647868 -5.95194379\n",
        " -6.64509097 -6.64509097 -5.95194379 -6.64509097 -6.64509097 -6.64509097\n",
        " -5.95194379 -5.95194379 -5.95194379 -4.08014161 -5.95194379 -5.54647868\n",
        " -5.95194379 -4.44786639 -6.64509097 -5.95194379 -5.95194379 -6.64509097\n",
        " -5.54647868 -5.95194379 -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -5.95194379 -6.64509097 -5.95194379 -6.64509097 -6.64509097 -6.64509097\n",
        " -4.2471957  -5.95194379 -5.95194379 -5.95194379 -6.64509097 -5.95194379\n",
        " -5.95194379 -5.95194379 -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -6.64509097 -6.64509097 -5.95194379 -6.64509097 -6.64509097 -5.95194379\n",
        " -5.95194379 -6.64509097 -5.95194379 -5.95194379 -6.64509097 -6.64509097\n",
        " -6.64509097 -5.95194379 -6.64509097 -6.64509097 -5.95194379 -6.64509097\n",
        " -6.64509097 -5.95194379 -5.95194379 -6.64509097 -6.64509097 -5.95194379\n",
        " -5.03565306 -5.95194379 -5.54647868 -5.95194379 -5.54647868 -5.95194379\n",
        " -5.54647868 -6.64509097 -5.95194379 -5.95194379 -5.25879661 -5.95194379\n",
        " -5.95194379 -5.25879661 -5.25879661 -5.95194379 -6.64509097 -4.8533315\n",
        " -6.64509097 -5.95194379 -5.95194379 -5.95194379 -5.54647868 -5.95194379\n",
        " -6.64509097 -5.25879661 -5.95194379 -5.95194379 -5.95194379 -6.64509097\n",
        " -6.64509097 -5.54647868 -6.64509097 -6.64509097 -5.95194379 -5.95194379\n",
        " -6.64509097 -5.95194379 -5.54647868 -5.95194379 -6.64509097 -5.95194379\n",
        " -5.95194379 -5.95194379 -6.64509097 -6.64509097 -5.95194379 -5.95194379\n",
        " -6.64509097 -5.54647868 -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -5.54647868 -5.95194379 -5.95194379 -6.64509097 -5.54647868 -5.95194379\n",
        " -5.03565306 -6.64509097 -5.95194379 -5.54647868 -6.64509097 -6.64509097\n",
        " -6.64509097 -5.95194379 -5.95194379 -6.64509097 -5.95194379 -5.25879661\n",
        " -5.95194379 -6.64509097 -6.64509097 -6.64509097 -6.64509097 -5.95194379\n",
        " -5.95194379 -6.64509097 -5.95194379 -5.95194379 -6.64509097 -5.54647868\n",
        " -6.64509097 -6.64509097 -6.64509097 -4.8533315  -5.95194379 -5.95194379\n",
        " -5.95194379 -5.95194379 -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -6.64509097 -6.64509097 -6.64509097 -5.95194379 -5.95194379 -5.95194379\n",
        " -6.64509097 -5.95194379 -5.95194379 -6.64509097 -6.64509097 -5.95194379\n",
        " -6.64509097 -6.64509097 -5.03565306 -6.64509097 -5.95194379 -5.95194379\n",
        " -5.95194379 -6.64509097 -4.16018432 -5.95194379 -5.95194379 -5.95194379\n",
        " -5.95194379 -6.64509097 -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -6.64509097 -6.64509097 -5.25879661 -6.64509097 -6.64509097 -5.95194379\n",
        " -6.64509097 -3.50959675 -6.64509097 -6.64509097 -6.64509097 -6.64509097\n",
        " -5.95194379 -4.8533315  -6.64509097 -5.95194379 -5.95194379 -5.95194379\n",
        " -5.25879661 -6.64509097 -6.64509097 -5.95194379 -5.95194379 -6.64509097\n",
        " -5.95194379 -5.95194379 -5.95194379 -6.64509097 -6.64509097 -6.64509097\n",
        " -6.64509097 -6.64509097 -6.64509097 -6.64509097 -6.64509097 -5.95194379\n",
        " -6.64509097 -6.64509097 -6.64509097 -5.95194379 -5.25879661 -6.64509097\n",
        " -5.95194379 -5.95194379 -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -6.64509097 -5.95194379 -6.64509097 -6.64509097 -5.95194379 -6.64509097\n",
        " -5.95194379 -6.64509097 -5.95194379 -6.64509097 -6.64509097 -5.54647868\n",
        " -5.54647868 -5.54647868 -5.95194379 -5.54647868 -5.25879661 -5.54647868\n",
        " -6.64509097 -5.95194379 -6.64509097 -5.54647868 -6.64509097 -5.25879661\n",
        " -6.64509097 -5.95194379 -6.64509097 -6.64509097 -6.64509097 -5.54647868\n",
        " -5.03565306 -6.64509097 -6.64509097 -6.64509097 -6.64509097 -5.95194379\n",
        " -5.95194379 -6.64509097 -5.95194379 -5.95194379 -5.95194379 -5.03565306\n",
        " -5.54647868 -5.95194379 -5.95194379 -6.64509097 -5.25879661 -6.64509097\n",
        " -5.54647868 -4.34250588 -5.95194379 -5.54647868 -5.95194379 -6.64509097\n",
        " -5.54647868 -6.64509097 -6.64509097 -5.03565306 -6.64509097 -5.95194379\n",
        " -6.64509097 -5.95194379 -6.64509097 -5.95194379 -5.25879661 -5.95194379\n",
        " -6.64509097 -6.64509097 -5.54647868 -6.64509097 -5.95194379 -6.64509097\n",
        " -5.25879661 -5.95194379 -6.64509097 -5.95194379 -5.54647868 -5.54647868\n",
        " -5.95194379 -4.8533315  -5.95194379 -5.25879661 -5.95194379 -6.64509097\n",
        " -6.64509097 -5.95194379 -5.95194379 -6.64509097 -6.64509097 -6.64509097\n",
        " -6.64509097 -5.95194379 -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -5.95194379 -5.95194379 -5.95194379 -5.95194379 -6.64509097 -6.64509097\n",
        " -6.64509097 -6.64509097 -6.64509097 -6.64509097 -5.95194379 -6.64509097\n",
        " -6.64509097 -5.95194379 -5.95194379 -6.64509097 -6.64509097 -5.95194379\n",
        " -5.95194379 -5.95194379 -5.25879661 -5.03565306 -5.95194379 -4.69918082\n",
        " -6.64509097 -5.95194379 -5.95194379 -5.95194379 -5.54647868 -6.64509097\n",
        " -6.64509097 -5.25879661 -4.8533315  -5.95194379 -6.64509097 -6.64509097\n",
        " -5.95194379 -6.64509097 -6.64509097 -6.64509097 -6.64509097 -5.95194379\n",
        " -5.95194379 -5.95194379 -6.64509097 -5.95194379 -5.95194379 -5.95194379\n",
        " -6.64509097 -5.95194379 -4.69918082 -5.95194379 -4.8533315  -6.64509097\n",
        " -6.64509097 -5.95194379 -6.64509097 -5.95194379 -5.95194379 -6.64509097\n",
        " -6.64509097 -6.64509097 -5.95194379 -5.95194379 -5.95194379 -5.54647868\n",
        " -6.64509097 -5.95194379 -3.06157203 -5.95194379 -5.54647868 -5.95194379\n",
        " -5.25879661 -5.25879661 -5.54647868 -6.64509097 -5.03565306 -6.64509097\n",
        " -6.64509097 -5.95194379 -6.64509097 -5.95194379 -6.64509097 -5.95194379\n",
        " -5.95194379 -5.95194379 -5.95194379 -6.64509097 -5.95194379 -6.64509097\n",
        " -5.25879661 -5.25879661 -6.64509097 -6.64509097 -6.64509097 -6.64509097\n",
        " -6.64509097 -6.64509097 -4.8533315  -6.64509097 -5.95194379 -6.64509097\n",
        " -5.95194379 -6.64509097 -5.95194379 -6.64509097 -6.64509097 -6.64509097\n",
        " -5.25879661 -5.95194379 -5.95194379 -6.64509097 -5.25879661 -6.64509097\n",
        " -6.64509097 -6.64509097 -6.64509097 -5.95194379 -5.54647868 -5.95194379\n",
        " -6.64509097 -6.64509097 -6.64509097 -5.95194379 -6.64509097 -5.95194379\n",
        " -5.54647868 -5.95194379 -6.64509097 -5.95194379 -5.95194379 -5.95194379\n",
        " -6.64509097 -5.25879661 -6.64509097 -4.69918082 -6.64509097 -6.64509097\n",
        " -5.95194379 -5.95194379 -4.44786639 -6.64509097 -5.95194379 -5.95194379\n",
        " -6.64509097 -6.64509097 -6.64509097 -5.95194379 -6.64509097 -5.95194379\n",
        " -5.95194379 -5.95194379 -6.64509097 -5.95194379 -6.64509097 -6.64509097\n",
        " -5.95194379 -5.95194379 -6.64509097 -5.25879661 -5.95194379 -5.95194379\n",
        " -6.64509097 -5.54647868 -6.64509097 -6.64509097 -6.64509097 -5.54647868\n",
        " -6.64509097 -5.95194379 -4.08014161 -6.64509097 -4.2471957  -5.95194379\n",
        " -6.64509097 -6.64509097 -5.03565306 -6.64509097 -6.64509097 -5.95194379\n",
        " -5.95194379 -6.64509097 -5.54647868 -6.64509097 -6.64509097 -5.25879661\n",
        " -4.69918082 -6.64509097 -6.64509097 -6.64509097 -5.95194379 -5.95194379\n",
        " -6.64509097 -5.95194379 -5.95194379 -6.64509097 -4.69918082 -6.64509097\n",
        " -6.64509097 -5.95194379 -5.95194379 -6.64509097 -6.64509097 -6.64509097\n",
        " -5.95194379 -6.64509097 -5.95194379 -5.95194379 -5.95194379 -6.64509097\n",
        " -5.95194379 -6.64509097 -5.95194379 -5.95194379 -5.95194379 -5.95194379\n",
        " -5.95194379 -6.64509097 -5.25879661 -6.64509097 -5.95194379 -5.25879661\n",
        " -6.64509097 -6.64509097 -5.95194379 -3.3492541  -5.54647868 -5.54647868\n",
        " -5.95194379 -5.95194379 -5.95194379 -5.95194379 -6.64509097 -6.64509097\n",
        " -5.95194379 -6.64509097 -6.64509097 -5.95194379 -5.95194379 -6.64509097\n",
        " -5.95194379 -6.64509097 -6.64509097 -5.95194379 -5.95194379 -5.95194379\n",
        " -5.95194379 -5.95194379]\n",
        "[-4.78749174 -6.39692966 -6.39692966 -6.39692966 -5.01063529 -5.70378247\n",
        " -6.39692966 -6.39692966 -6.39692966 -4.19970508 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.70378247\n",
        " -6.39692966 -6.39692966 -5.70378247 -5.01063529 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.01063529 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.29831737\n",
        " -6.39692966 -5.70378247 -5.01063529 -6.39692966 -6.39692966 -5.29831737\n",
        " -6.39692966 -5.29831737 -6.39692966 -6.39692966 -5.70378247 -6.39692966\n",
        " -5.70378247 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -5.70378247 -4.78749174 -6.39692966 -6.39692966 -5.70378247\n",
        " -5.70378247 -6.39692966 -6.39692966 -5.01063529 -6.39692966 -5.01063529\n",
        " -6.39692966 -6.39692966 -5.70378247 -6.39692966 -6.39692966 -3.45249068\n",
        " -5.70378247 -6.39692966 -6.39692966 -6.39692966 -5.29831737 -6.39692966\n",
        " -6.39692966 -6.39692966 -4.60517019 -5.29831737 -5.70378247 -5.29831737\n",
        " -6.39692966 -6.39692966 -5.70378247 -5.01063529 -5.29831737 -6.39692966\n",
        " -5.01063529 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -5.01063529 -6.39692966 -4.19970508 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -5.01063529 -4.60517019 -6.39692966 -6.39692966 -6.39692966\n",
        " -4.78749174 -5.70378247 -6.39692966 -6.39692966 -5.01063529 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -4.78749174\n",
        " -6.39692966 -5.29831737 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.29831737 -6.39692966 -4.78749174\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -4.78749174 -6.39692966 -6.39692966 -6.39692966\n",
        " -4.78749174 -5.29831737 -6.39692966 -6.39692966 -5.70378247 -6.39692966\n",
        " -6.39692966 -5.70378247 -6.39692966 -6.39692966 -4.45101951 -4.78749174\n",
        " -5.70378247 -6.39692966 -5.70378247 -5.70378247 -6.39692966 -6.39692966\n",
        " -5.29831737 -6.39692966 -6.39692966 -5.70378247 -5.70378247 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.70378247 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -4.45101951 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -5.70378247 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -4.78749174 -6.39692966 -4.78749174 -4.78749174 -6.39692966 -6.39692966\n",
        " -5.01063529 -6.39692966 -6.39692966 -6.39692966 -5.01063529 -6.39692966\n",
        " -6.39692966 -6.39692966 -5.29831737 -5.70378247 -5.01063529 -6.39692966\n",
        " -4.09434456 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.01063529 -6.39692966 -6.39692966\n",
        " -6.39692966 -5.70378247 -6.39692966 -6.39692966 -5.01063529 -5.70378247\n",
        " -5.70378247 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -5.70378247 -5.70378247 -6.39692966 -4.78749174 -6.39692966\n",
        " -6.39692966 -5.29831737 -6.39692966 -6.39692966 -5.29831737 -6.39692966\n",
        " -5.29831737 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -4.78749174 -5.70378247 -6.39692966 -6.39692966 -6.39692966 -5.29831737\n",
        " -5.29831737 -6.39692966 -6.39692966 -5.70378247 -5.01063529 -6.39692966\n",
        " -6.39692966 -5.29831737 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -5.01063529 -5.70378247 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -5.70378247 -6.39692966 -6.39692966 -5.70378247 -5.29831737 -6.39692966\n",
        " -5.29831737 -3.75787233 -5.29831737 -4.31748811 -5.70378247 -3.99903438\n",
        " -6.39692966 -4.60517019 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -4.78749174 -5.29831737 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.70378247 -5.01063529 -5.70378247\n",
        " -5.70378247 -5.29831737 -6.39692966 -5.70378247 -5.70378247 -6.39692966\n",
        " -5.01063529 -5.29831737 -4.60517019 -6.39692966 -6.39692966 -5.29831737\n",
        " -6.39692966 -5.29831737 -6.39692966 -6.39692966 -5.29831737 -6.39692966\n",
        " -5.01063529 -6.39692966 -4.78749174 -5.01063529 -6.39692966 -5.29831737\n",
        " -6.39692966 -5.70378247 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.01063529 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -4.78749174 -5.01063529\n",
        " -5.01063529 -6.39692966 -4.31748811 -5.01063529 -5.01063529 -6.39692966\n",
        " -6.39692966 -5.29831737 -6.39692966 -5.01063529 -5.70378247 -6.39692966\n",
        " -6.39692966 -5.01063529 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.01063529 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.70378247 -6.39692966\n",
        " -6.39692966 -6.39692966 -5.29831737 -6.39692966 -6.39692966 -6.39692966\n",
        " -4.78749174 -5.29831737 -6.39692966 -5.70378247 -6.39692966 -5.70378247\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.70378247\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.29831737 -5.70378247 -5.29831737\n",
        " -4.60517019 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.29831737 -6.39692966\n",
        " -6.39692966 -4.60517019 -6.39692966 -6.39692966 -6.39692966 -5.01063529\n",
        " -5.70378247 -6.39692966 -6.39692966 -5.70378247 -5.70378247 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.01063529\n",
        " -5.01063529 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.70378247\n",
        " -5.70378247 -6.39692966 -6.39692966 -6.39692966 -5.70378247 -6.39692966\n",
        " -6.39692966 -5.70378247 -4.45101951 -5.01063529 -4.78749174 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.29831737 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -4.78749174 -4.78749174 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -5.01063529 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.70378247 -6.39692966 -5.70378247\n",
        " -5.29831737 -6.39692966 -5.01063529 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.29831737\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.70378247 -4.78749174\n",
        " -5.29831737 -6.39692966 -6.39692966 -5.01063529 -6.39692966 -5.70378247\n",
        " -6.39692966 -4.45101951 -6.39692966 -6.39692966 -6.39692966 -5.29831737\n",
        " -6.39692966 -6.39692966 -6.39692966 -5.29831737 -6.39692966 -5.70378247\n",
        " -6.39692966 -5.70378247 -5.70378247 -6.39692966 -6.39692966 -6.39692966\n",
        " -5.70378247 -5.70378247 -4.78749174 -6.39692966 -5.70378247 -6.39692966\n",
        " -6.39692966 -6.39692966 -5.70378247 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.01063529 -5.29831737\n",
        " -6.39692966 -6.39692966 -4.19970508 -6.39692966 -6.39692966 -6.39692966\n",
        " -5.70378247 -4.78749174 -4.78749174 -6.39692966 -5.29831737 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.01063529 -5.70378247\n",
        " -6.39692966 -6.39692966 -5.01063529 -6.39692966 -6.39692966 -6.39692966\n",
        " -5.29831737 -6.39692966 -5.70378247 -5.01063529 -4.78749174 -6.39692966\n",
        " -5.01063529 -6.39692966 -6.39692966 -6.39692966 -4.78749174 -6.39692966\n",
        " -4.78749174 -6.39692966 -6.39692966 -5.70378247 -5.70378247 -6.39692966\n",
        " -6.39692966 -5.29831737 -6.39692966 -5.70378247 -4.78749174 -6.39692966\n",
        " -6.39692966 -5.29831737 -4.60517019 -5.70378247 -6.39692966 -6.39692966\n",
        " -4.78749174 -6.39692966 -6.39692966 -5.29831737 -5.29831737 -5.70378247\n",
        " -5.70378247 -6.39692966 -6.39692966 -5.70378247 -6.39692966 -4.45101951\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -5.70378247\n",
        " -6.39692966 -4.31748811 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -5.29831737 -5.70378247 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -5.29831737 -6.39692966 -6.39692966 -4.31748811 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966 -6.39692966\n",
        " -6.39692966 -4.45101951 -6.39692966 -6.39692966 -6.39692966 -5.01063529\n",
        " -5.29831737 -5.70378247 -5.70378247 -5.70378247 -6.39692966 -6.39692966\n",
        " -6.39692966 -6.39692966]\n",
        "0.475\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 6. Test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errorCount = 0\n",
      "for docIndex in testSet:        # classify the remaining items\n",
      "    wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
      "    if classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
      "        errorCount += 1\n",
      "        print \"classification error\",docList[docIndex]\n",
      "\n",
      "print 'the error rate is: ',float(errorCount)/len(testSet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "p1(Abusive) : -162.792400\n",
        "\n",
        "p2(Not abusive) : -158.027895\n",
        "\n",
        "p1(Abusive) : -154.779950\n",
        "\n",
        "p2(Not abusive) : -217.661212\n",
        "\n",
        "p1(Abusive) : -558.948724\n",
        "\n",
        "p2(Not abusive) : -531.633809\n",
        "\n",
        "p1(Abusive) : -149.187610\n",
        "\n",
        "p2(Not abusive) : -138.389157\n",
        "\n",
        "p1(Abusive) : -134.457891\n",
        "\n",
        "p2(Not abusive) : -166.697709\n",
        "\n",
        "p1(Abusive) : -201.489831\n",
        "\n",
        "p2(Not abusive) : -198.005510\n",
        "\n",
        "classification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
        "p1(Abusive) : -131.570349\n",
        "\n",
        "p2(Not abusive) : -167.283931\n",
        "\n",
        "p1(Abusive) : -197.465771\n",
        "\n",
        "p2(Not abusive) : -264.753514\n",
        "\n",
        "p1(Abusive) : -323.344250\n",
        "\n",
        "p2(Not abusive) : -300.616509\n",
        "\n",
        "p1(Abusive) : -154.779950\n",
        "\n",
        "p2(Not abusive) : -217.661212\n",
        "\n",
        "the error rate is:  0.1\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}